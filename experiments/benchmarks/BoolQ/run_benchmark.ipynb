{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# get dataset\n",
    "ds = load_dataset(\"google/boolq\")\n",
    "trainset_ds = ds[\"train\"]\n",
    "trainset_raw = trainset_ds.to_list()\n",
    "\n",
    "testset_ds = ds[\"validation\"]\n",
    "testset_raw = testset_ds.to_list()\n",
    "random.seed(1)\n",
    "\n",
    "trainset_random = random.sample(trainset_raw, 200)\n",
    "testset_random = random.sample(testset_raw, 100)\n",
    "\n",
    "from ape.common.types import DatasetItem\n",
    "\n",
    "trainset = [DatasetItem(inputs={\"question\": item[\"question\"], \"passage\": item[\"passage\"]}, outputs={\"answer\": item[\"answer\"]}) for item in trainset_random]\n",
    "testset = [DatasetItem(inputs={\"question\": item[\"question\"], \"passage\": item[\"passage\"]}, outputs={\"answer\": item[\"answer\"]}) for item in testset_random]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prompt to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import Prompt\n",
    "\n",
    "# define prompt\n",
    "prompt_with_passage = \"\"\"\\\n",
    "For given question and passage, return True or False.\n",
    "\n",
    "respond in JSON format:\n",
    "{{\n",
    "    \"thought\": \"<thought>\",\n",
    "    \"answer\": \"<True or False>\"\n",
    "}}\n",
    "\n",
    "question: {question}\n",
    "passage: {passage}\n",
    "\"\"\"\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\", \"json_schema\": {\n",
    "        \"name\": \"Prediction\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"thought\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The reasoning process of the prediction\"\n",
    "                },\n",
    "                \"answer\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The prediction of the given question\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"thought\", \"answer\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "student_prompt = Prompt(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_with_passage},\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    name=\"Boolean Question Answer Bot\",\n",
    "    response_format=response_format,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Generator, Metric, and Global Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common.generator import BaseGenerator\n",
    "from ape.common.metric import BaseMetric\n",
    "from ape.common.global_metric import BaseGlobalMetric\n",
    "from ape.common.types import MetricResult, GlobalMetricResult\n",
    "\n",
    "# define generator, metric, global metric\n",
    "openai = AsyncOpenAI()\n",
    "\n",
    "class Classifier(BaseGenerator):\n",
    "    async def generate(\n",
    "        self,\n",
    "        prompt: Prompt,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Union[Dict[str, Any], str]:\n",
    "        try:\n",
    "            messages = prompt.format(**inputs).messages\n",
    "            response = await openai.chat.completions.create(\n",
    "                model=prompt.model,\n",
    "                messages=messages,\n",
    "                response_format=prompt.response_format,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            simulation_cost.append(response.usage.model_dump())\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {\n",
    "                \"thought\": \"error\",\n",
    "                \"answer\": None,\n",
    "            }\n",
    "    \n",
    "class BoolQMetric(BaseMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        dataset_item: DatasetItem,\n",
    "        pred: Dict[str, Any],\n",
    "    ) -> MetricResult:\n",
    "        try:\n",
    "            pred_answer = pred[\"answer\"]\n",
    "            gold_answer = dataset_item[\"outputs\"][\"answer\"]\n",
    "            if pred_answer == gold_answer:\n",
    "                return MetricResult(\n",
    "                    score=1.0,\n",
    "                )\n",
    "            else:\n",
    "                return MetricResult(\n",
    "                    score=0.0,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            # print(pred)\n",
    "            return MetricResult(\n",
    "                score=0.0,\n",
    "            )\n",
    "\n",
    "class GlobalBoolQMetric(BaseGlobalMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        results: List[MetricResult],\n",
    "    ) -> GlobalMetricResult:\n",
    "        try:\n",
    "            scores = [result.score for result in results]\n",
    "            return GlobalMetricResult(\n",
    "                score=sum(scores) / len(scores) if len(results) > 0 else 0.0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # print(\"Error in GlobalEmotionMetric: \", e)\n",
    "            return GlobalMetricResult(\n",
    "                score=0.0,\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Trainer & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.core.trainer import (\n",
    "    TextGradientTrainer,\n",
    "    ExpelTrainer,\n",
    "    FewShotTrainer,\n",
    "    EvoPromptTrainer,\n",
    "    DspyMiproTrainer,\n",
    "    OptunaTrainer,\n",
    ")\n",
    "\n",
    "# define trainer \n",
    "trainer = FewShotTrainer(\n",
    "    generator=Classifier(),\n",
    "    metric=BoolQMetric(),\n",
    "    global_metric=GlobalBoolQMetric(),\n",
    "    testmode=True # If True, trainer will run prompts for validation set and save results.\n",
    ")\n",
    "\n",
    "# run trainer\n",
    "optimized_prompt, report = await trainer.train(\n",
    "    prompt=student_prompt,\n",
    "    trainset=trainset,\n",
    "    valset=testset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimized prompt\n",
    "for message in optimized_prompt.messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Benchmark Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize experiment results\n",
    "def visualize_scores(report):\n",
    "    scores = report.scores\n",
    "    trainset_scores = [score[\"score\"] for score in scores]\n",
    "    valset_scores = [score[\"val_score\"] for score in scores]\n",
    "    iterations = range(1, len(trainset_scores) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, trainset_scores, label='Training Set', marker='o')\n",
    "    plt.plot(iterations, valset_scores, label='Validation Set', marker='s')\n",
    "    \n",
    "    for i, (train_score, val_score) in enumerate(zip(trainset_scores, valset_scores)):\n",
    "        plt.text(iterations[i], train_score, f'{train_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='blue')\n",
    "        plt.text(iterations[i], val_score, f'{val_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='green')\n",
    "\n",
    "    plt.title('Training and Validation Scores over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scores(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
