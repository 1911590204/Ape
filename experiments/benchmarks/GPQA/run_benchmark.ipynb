{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# get dataset\n",
    "ds = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\")\n",
    "ds = ds[\"train\"]\n",
    "ds = ds.to_list()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(ds)\n",
    "trainset = ds[:100]\n",
    "testset = ds[100:201]\n",
    "\n",
    "from ape.common.types import DatasetItem\n",
    "\n",
    "trainset = [DatasetItem(inputs={\"question\": item[\"Question\"]}, outputs={\"thought\": item[\"Explanation\"], \"answer\": item[\"Correct Answer\"]}) for item in trainset]\n",
    "testset = [DatasetItem(inputs={\"question\": item[\"Question\"]}, outputs={\"thought\": item[\"Explanation\"], \"answer\": item[\"Correct Answer\"]}) for item in testset]\n",
    "\n",
    "testset = [data for data in testset if \"ATCG\" not in data[\"outputs\"][\"answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prompt to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import Prompt\n",
    "\n",
    "# define prompt\n",
    "system_prompt = \"\"\"\\\n",
    "For given science question, solve it step by step.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "You MUST respond in JSON format with the following fields:\n",
    "thought: the reasoning process of the problem solving.\n",
    "answer: only return the answer without any explanation.\n",
    "\"\"\"\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"json_schema\", \n",
    "    \"json_schema\": {\n",
    "        \"name\": \"ScienceProblemSolving\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"thought\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The reasoning process of the problem solving\"\n",
    "                },\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The answer to the question\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"thought\", \"answer\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "student_prompt = Prompt(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    name=\"Science Problem Solver\",\n",
    "    response_format=json_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Generator, Metric, and Global Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "from ape.common.generator import BaseGenerator\n",
    "from ape.common.metric import BaseMetric\n",
    "from ape.common.global_metric import BaseGlobalMetric\n",
    "from ape.common.types import MetricResult, GlobalMetricResult\n",
    "\n",
    "# define generator, metric, global metric\n",
    "openai = AsyncOpenAI()\n",
    "\n",
    "class ScienceSolver(BaseGenerator):\n",
    "    async def generate(\n",
    "        self,\n",
    "        prompt: Prompt,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Union[Dict[str, Any], str]:\n",
    "        retry_count = 0\n",
    "        messages = prompt.format(**inputs).messages\n",
    "        model = prompt.model\n",
    "        response_format = prompt.response_format\n",
    "\n",
    "        while retry_count < 3:\n",
    "            stream_response = None\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                stream_response = await asyncio.wait_for(\n",
    "                    openai.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                        response_format=response_format,\n",
    "                        temperature=0.0,\n",
    "                        stream=True,\n",
    "                        frequency_penalty=0.1\n",
    "                    ),\n",
    "                    timeout=10.0\n",
    "                )\n",
    "                full_response = \"\"\n",
    "                async for chunk in stream_response:\n",
    "                    if time.time() - start_time > 30.0:\n",
    "                        raise Exception(\"TimeoutError\")\n",
    "                    \n",
    "                    if len(chunk.choices) == 0:\n",
    "                        continue\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        full_response += chunk.choices[0].delta.content\n",
    "\n",
    "                return json.loads(full_response)\n",
    "\n",
    "            except asyncio.TimeoutError:\n",
    "                # print(\"TimeoutError\")\n",
    "                retry_count += 1\n",
    "                if retry_count == 3:\n",
    "                    return {\n",
    "                        \"thought\": \"error: stream timeout\",\n",
    "                        \"answer\": \"\",\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                # print(f\"Other Error, {e}\")\n",
    "                retry_count += 1\n",
    "                if retry_count == 3:\n",
    "                    return {\n",
    "                        \"thought\": f\"error: {str(e)}\",\n",
    "                        \"answer\": \"\",\n",
    "                    }\n",
    "\n",
    "        return {\n",
    "            \"thought\": \"error: max retries reached\",\n",
    "            \"answer\": \"\",\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "eval_json_schema = {\n",
    "    \"type\": \"json_schema\", \n",
    "    \"json_schema\": {\n",
    "        \"name\": \"ScienceQuestionSolvingEvaluation\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"thought\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The reasoning process of the problem solving evaluation\"\n",
    "                },\n",
    "                \"correctness\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The correctness of the problem solving\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"thought\", \"correctness\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class GPQAMetric(BaseMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        dataset_item: DatasetItem,\n",
    "        pred: Dict[str, Any],\n",
    "    ) -> MetricResult:\n",
    "        retry_count = 0\n",
    "        while retry_count < 3:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                stream_response = await asyncio.wait_for(\n",
    "                    openai.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"system\",\n",
    "                                \"content\": \"\"\"\\\n",
    "        YOU ARE one of the GREATEST scientists. You are intelligent and rational. You are prudent and cautious. Your mastery over science is unparalleled. You THINK NATURAL, BROAD AND DEEP. Let's think step by step. \n",
    "        Your job is to judge whether the \"final_answer\" is correct based on \"ground_truth_answer\", do not be strict on the format, but check the content. Notice that unsolved half results are not Correct. \n",
    "        Question: {question_content}\n",
    "        Is the final_answer correct, given the ground truth answer? Reply with Correct, Wrong or Unknown. \n",
    "        \"final_answer\": \"{final_answer}\", \"ground_truth_answer\": \"{ground_truth_answer}\"\n",
    "\n",
    "        You MUST respond in JSON format like below:\n",
    "        {{\n",
    "            \"thought\": \"...\",\n",
    "            \"correctness\": \"<correctness>\", One of \"Correct\", \"Wrong\", \"Unknown\"\n",
    "        }}\n",
    "        \"\"\".format(question_content=dataset_item[\"inputs\"][\"question\"], final_answer=pred[\"answer\"], ground_truth_answer=dataset_item[\"outputs\"][\"answer\"])\n",
    "                            }\n",
    "                        ],\n",
    "                        response_format=eval_json_schema,\n",
    "                        temperature=0.0,\n",
    "                        stream=True,\n",
    "                        frequency_penalty=0.1\n",
    "                    ),\n",
    "                    timeout=10.0\n",
    "                )\n",
    "                full_response = \"\"\n",
    "                async for chunk in stream_response:\n",
    "                    if time.time() - start_time > 30.0:\n",
    "                        raise Exception(\"TimeoutError\")\n",
    "                    if len(chunk.choices) == 0:\n",
    "                        continue\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        full_response += chunk.choices[0].delta.content\n",
    "\n",
    "                res_json = json.loads(full_response)\n",
    "                if res_json[\"correctness\"] == \"Correct\":\n",
    "                    return MetricResult(score=1.0)\n",
    "                else:\n",
    "                    return MetricResult(score=0.0)\n",
    "\n",
    "            except asyncio.TimeoutError:\n",
    "                # print(\"TimeoutError\")\n",
    "                retry_count += 1\n",
    "                if retry_count == 3:\n",
    "                    return MetricResult(score=0.0)\n",
    "            except Exception as e:\n",
    "                # print(f\"Other Error: {e}\")\n",
    "                retry_count += 1\n",
    "                if retry_count == 3:\n",
    "                    return MetricResult(score=0.0)\n",
    "\n",
    "        return MetricResult(score=0.0)\n",
    "\n",
    "class GlobalGPQAMetric(BaseGlobalMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        results: List[MetricResult],\n",
    "    ) -> GlobalMetricResult:\n",
    "        try:\n",
    "            scores = [result.score for result in results]\n",
    "            return GlobalMetricResult(\n",
    "                score=sum(scores) / len(scores) if len(results) > 0 else 0.0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            return GlobalMetricResult(\n",
    "                score=0.0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Trainer & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.core.trainer import (\n",
    "    TextGradientTrainer,\n",
    "    ExpelTrainer,\n",
    "    FewShotTrainer,\n",
    "    EvoPromptTrainer,\n",
    "    DspyMiproTrainer,\n",
    "    OptunaTrainer,\n",
    ")\n",
    "\n",
    "# define trainer \n",
    "trainer = FewShotTrainer(\n",
    "    generator=ScienceSolver(),\n",
    "    metric=GPQAMetric(),\n",
    "    global_metric=GlobalGPQAMetric(),\n",
    "    testmode=True # If True, trainer will run prompts for validation set and save results.\n",
    ")\n",
    "\n",
    "# run trainer\n",
    "optimized_prompt, report = await trainer.train(\n",
    "    prompt=student_prompt,\n",
    "    trainset=trainset,\n",
    "    valset=testset,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimized prompt\n",
    "for message in optimized_prompt.messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Benchmark Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize experiment results\n",
    "def visualize_scores(report):\n",
    "    scores = report.scores\n",
    "    trainset_scores = [score[\"score\"] for score in scores]\n",
    "    valset_scores = [score[\"val_score\"] for score in scores]\n",
    "    iterations = range(1, len(trainset_scores) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, trainset_scores, label='Training Set', marker='o')\n",
    "    plt.plot(iterations, valset_scores, label='Validation Set', marker='s')\n",
    "    \n",
    "    for i, (train_score, val_score) in enumerate(zip(trainset_scores, valset_scores)):\n",
    "        plt.text(iterations[i], train_score, f'{train_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='blue')\n",
    "        plt.text(iterations[i], val_score, f'{val_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='green')\n",
    "\n",
    "    plt.title('Training and Validation Scores over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scores(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
