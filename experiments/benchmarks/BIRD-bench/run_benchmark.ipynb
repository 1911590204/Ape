{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Dataset\n",
    "\n",
    "You must download the train dataset from [BIRD-bench](https://bird-bench.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "db_table_map = {\n",
    "    \"works_cycles\": [\n",
    "        \"CountryRegion\",\n",
    "        \"Culture\",\n",
    "        \"Currency\",\n",
    "        \"CountryRegionCurrency\",\n",
    "        \"Person\",\n",
    "        \"BusinessEntityContact\",\n",
    "        \"EmailAddress\",\n",
    "        \"Employee\",\n",
    "        \"Password\",\n",
    "        \"PersonCreditCard\",\n",
    "        \"ProductCategory\",\n",
    "        \"ProductDescription\",\n",
    "        \"ProductModel\",\n",
    "        \"ProductModelProductDescriptionCulture\",\n",
    "        \"ProductPhoto\",\n",
    "        \"ProductSubcategory\",\n",
    "        \"SalesReason\",\n",
    "        \"SalesTerritory\",\n",
    "        \"SalesPerson\",\n",
    "        \"SalesPersonQuotaHistory\",\n",
    "        \"SalesTerritoryHistory\",\n",
    "        \"ScrapReason\",\n",
    "        \"Shift\",\n",
    "        \"ShipMethod\",\n",
    "        \"SpecialOffer\",\n",
    "        \"BusinessEntityAddress\",\n",
    "        \"SalesTaxRate\",\n",
    "        \"Store\",\n",
    "        \"SalesOrderHeaderSalesReason\",\n",
    "        \"TransactionHistoryArchive\",\n",
    "        \"UnitMeasure\",\n",
    "        \"ProductCostHistory\",\n",
    "        \"ProductDocument\",\n",
    "        \"ProductInventory\",\n",
    "        \"ProductProductPhoto\",\n",
    "        \"ProductReview\",\n",
    "        \"ShoppingCartItem\",\n",
    "        \"SpecialOfferProduct\",\n",
    "        \"SalesOrderDetail\",\n",
    "        \"TransactionHistory\",\n",
    "        \"Vendor\",\n",
    "        \"ProductVendor\",\n",
    "        \"PurchaseOrderHeader\",\n",
    "        \"PurchaseOrderDetail\",\n",
    "        \"WorkOrder\",\n",
    "        \"WorkOrderRouting\",\n",
    "        \"Customer\",\n",
    "        \"ProductListPriceHistory\",\n",
    "        \"Address\",\n",
    "        \"AddressType\",\n",
    "        \"BillOfMaterials\",\n",
    "        \"BusinessEntity\",\n",
    "        \"ContactType\",\n",
    "        \"CurrencyRate\",\n",
    "        \"Department\",\n",
    "        \"EmployeeDepartmentHistory\",\n",
    "        \"EmployeePayHistory\",\n",
    "        \"JobCandidate\",\n",
    "        \"Location\",\n",
    "        \"PhoneNumberType\",\n",
    "        \"Product\",\n",
    "        \"Document\",\n",
    "        \"StateProvince\",\n",
    "        \"CreditCard\",\n",
    "        \"SalesOrderHeader\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def nice_look_table(column_names: list, values: list):\n",
    "    rows = []\n",
    "    # Determine the maximum width of each column\n",
    "    widths = [\n",
    "        max(len(str(value[i])) for value in values + [column_names])\n",
    "        for i in range(len(column_names))\n",
    "    ]\n",
    "\n",
    "    # Print the column names\n",
    "    header = \"\".join(\n",
    "        f\"{column.rjust(width)} \" for column, width in zip(column_names, widths)\n",
    "    )\n",
    "    # print(header)\n",
    "    # Print the values\n",
    "    for value in values:\n",
    "        row = \"\".join(f\"{str(v).rjust(width)} \" for v, width in zip(value, widths))\n",
    "        rows.append(row)\n",
    "    rows = \"\\n\".join(rows)\n",
    "    final_output = header + \"\\n\" + rows\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def generate_schema_prompt_sqlite(db_path, num_rows=None):\n",
    "    # extract create ddls\n",
    "    \"\"\"\n",
    "    :param root_place:\n",
    "    :param db_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    full_schema_prompt_list = []\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    schemas = {}\n",
    "    for table in tables:\n",
    "        if table == \"sqlite_sequence\":\n",
    "            continue\n",
    "        cursor.execute(\n",
    "            \"SELECT sql FROM sqlite_master WHERE type='table' AND name='{}';\".format(\n",
    "                table[0]\n",
    "            )\n",
    "        )\n",
    "        create_prompt = cursor.fetchone()[0]\n",
    "        schemas[table[0]] = create_prompt\n",
    "        if num_rows:\n",
    "            cur_table = table[0]\n",
    "            if cur_table in [\"order\", \"by\", \"group\"]:\n",
    "                cur_table = \"`{}`\".format(cur_table)\n",
    "\n",
    "            cursor.execute(\"SELECT * FROM {} LIMIT {}\".format(cur_table, num_rows))\n",
    "            column_names = [description[0] for description in cursor.description]\n",
    "            values = cursor.fetchall()\n",
    "            rows_prompt = nice_look_table(column_names=column_names, values=values)\n",
    "            verbose_prompt = \"/* \\n {} example rows: \\n SELECT * FROM {} LIMIT {}; \\n {} \\n */\".format(\n",
    "                num_rows, cur_table, num_rows, rows_prompt\n",
    "            )\n",
    "            schemas[table[0]] = \"{} \\n {}\".format(create_prompt, verbose_prompt)\n",
    "\n",
    "    for k, v in schemas.items():\n",
    "        full_schema_prompt_list.append(v)\n",
    "\n",
    "    schema_prompt = \"\\n\\n\".join(full_schema_prompt_list)\n",
    "\n",
    "    return schema_prompt\n",
    "\n",
    "# Basic Utilities\n",
    "def load_json(dir):\n",
    "    with open(dir, \"r\") as j:\n",
    "        contents = json.loads(j.read())\n",
    "    return contents\n",
    "\n",
    "def connect_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def execute_sql(sql, db_path, return_time=False):\n",
    "    # Connect to the database\n",
    "    conn = connect_db(db_path)\n",
    "    start_time = time.time()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    res = cursor.fetchall()\n",
    "    conn.close()  # Don't forget to close the connection!\n",
    "    exec_time = time.time() - start_time\n",
    "    if return_time:\n",
    "        return exec_time\n",
    "\n",
    "    return res\n",
    "\n",
    "# Calculate exact match\n",
    "def calculate_exact_match(predicted_res, ground_truth_res):\n",
    "    res = 0\n",
    "    if set(predicted_res) == set(ground_truth_res):\n",
    "        res = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculate_row_match(predicted_row, ground_truth_row):\n",
    "    \"\"\"\n",
    "    Calculate the matching percentage for a single row.\n",
    "\n",
    "    Args:\n",
    "    predicted_row (tuple): The predicted row values.\n",
    "    ground_truth_row (tuple): The actual row values from ground truth.\n",
    "\n",
    "    Returns:\n",
    "    float: The match percentage (0 to 1 scale).\n",
    "    \"\"\"\n",
    "    total_columns = len(ground_truth_row)\n",
    "    matches = 0\n",
    "    element_in_pred_only = 0\n",
    "    element_in_truth_only = 0\n",
    "    for pred_val in predicted_row:\n",
    "        if pred_val in ground_truth_row:\n",
    "            matches += 1\n",
    "        else:\n",
    "            element_in_pred_only += 1\n",
    "    for truth_val in ground_truth_row:\n",
    "        if truth_val not in predicted_row:\n",
    "            element_in_truth_only += 1\n",
    "    match_percentage = matches / total_columns\n",
    "    pred_only_percentage = element_in_pred_only / total_columns\n",
    "    truth_only_percentage = element_in_truth_only / total_columns\n",
    "    return match_percentage, pred_only_percentage, truth_only_percentage\n",
    "\n",
    "# Calculate F1 score\n",
    "def calculate_f1_score(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score based on sets of predicted results and ground truth results,\n",
    "    where each element (tuple) represents a row from the database with multiple columns.\n",
    "\n",
    "    Args:\n",
    "    predicted (set of tuples): Predicted results from SQL query.\n",
    "    ground_truth (set of tuples): Actual results expected (ground truth).\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated F1 score.\n",
    "    \"\"\"\n",
    "    # if both predicted and ground_truth are empty, return 1.0 for f1_score\n",
    "    if not predicted and not ground_truth:\n",
    "        return 1.0\n",
    "\n",
    "    # Drop duplicates\n",
    "    predicted_set = set(predicted) if predicted else set()\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    # convert back to list\n",
    "    predicted = list(predicted_set)\n",
    "    ground_truth = list(ground_truth_set)\n",
    "\n",
    "    # Calculate matching scores for each possible pair\n",
    "    match_scores = []\n",
    "    pred_only_scores = []\n",
    "    truth_only_scores = []\n",
    "    for i, gt_row in enumerate(ground_truth):\n",
    "        # rows only in the ground truth results\n",
    "        if i >= len(predicted):\n",
    "            match_scores.append(0)\n",
    "            truth_only_scores.append(1)\n",
    "            continue\n",
    "        pred_row = predicted[i]\n",
    "        match_score, pred_only_score, truth_only_score = calculate_row_match(\n",
    "            pred_row, gt_row\n",
    "        )\n",
    "        match_scores.append(match_score)\n",
    "        pred_only_scores.append(pred_only_score)\n",
    "        truth_only_scores.append(truth_only_score)\n",
    "\n",
    "    # rows only in the predicted results\n",
    "    for i in range(len(predicted) - len(ground_truth)):\n",
    "        match_scores.append(0)\n",
    "        pred_only_scores.append(1)\n",
    "        truth_only_scores.append(0)\n",
    "\n",
    "    tp = sum(match_scores)\n",
    "    fp = sum(pred_only_scores)\n",
    "    fn = sum(truth_only_scores)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "    f1_score = (\n",
    "        2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    )\n",
    "    return f1_score\n",
    "\n",
    "# Caculate verocity efficiency score\n",
    "def clean_abnormal(input):\n",
    "    input = np.asarray(input)\n",
    "    processed_list = []\n",
    "    mean = np.mean(input, axis=0)\n",
    "    std = np.std(input, axis=0)\n",
    "    for x in input:\n",
    "        if x < mean + 3 * std and x > mean - 3 * std:\n",
    "            processed_list.append(x)\n",
    "    return processed_list\n",
    "\n",
    "\n",
    "def calculate_velocity_efficiency_score(\n",
    "    predicted_sql, ground_truth, db_path, iterate_num\n",
    "):\n",
    "    diff_list = []\n",
    "    predicted_res = execute_sql(predicted_sql, db_path)\n",
    "    ground_truth_res = execute_sql(ground_truth, db_path)\n",
    "    reward = 0\n",
    "    time_ratio = 0\n",
    "    if set(predicted_res) == set(ground_truth_res):\n",
    "        for _ in range(iterate_num):\n",
    "            predicted_time = execute_sql(\n",
    "                predicted_sql, db_path, return_time=True\n",
    "            )\n",
    "            ground_truth_time = execute_sql(\n",
    "                ground_truth, db_path, return_time=True\n",
    "            )\n",
    "            diff_list.append(ground_truth_time / predicted_time)\n",
    "        processed_diff_list = clean_abnormal(diff_list)\n",
    "        time_ratio = sum(processed_diff_list) / len(processed_diff_list)\n",
    "    if time_ratio == 0:\n",
    "        reward = 0\n",
    "    elif time_ratio >= 2:\n",
    "        reward = 1.25\n",
    "    elif time_ratio >= 1 and time_ratio < 2:\n",
    "        reward = 1\n",
    "    elif time_ratio >= 0.5 and time_ratio < 1:\n",
    "        reward = 0.75\n",
    "    elif time_ratio >= 0.25 and time_ratio < 0.5:\n",
    "        reward = 0.5\n",
    "    else:\n",
    "        reward = 0.25\n",
    "    # return time_ratio\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# get dataset\n",
    "# For tutorial, we will use openai/gsm8k\n",
    "trainset = []\n",
    "with open(\"train/train.json\", \"r\") as f:\n",
    "    trainset = json.load(f)\n",
    "\n",
    "train_tables = []\n",
    "with open(\"train/train_tables.json\", \"r\") as f:\n",
    "    train_tables = json.load(f)\n",
    "\n",
    "# select 'works_cycles' database\n",
    "works_cycles_trainset = [item for item in trainset if item[\"db_id\"] == \"works_cycles\"]\n",
    "works_cycles_train_tables = [table for table in train_tables if table[\"db_id\"] == \"works_cycles\"]\n",
    "\n",
    "# save\n",
    "with open(\"train/works_cycles_train.json\", \"w\") as f:\n",
    "    json.dump(works_cycles_trainset, f)\n",
    "with open(\"train/works_cycles_train_tables.json\", \"w\") as f:\n",
    "    json.dump(works_cycles_train_tables, f)\n",
    "\n",
    "dataset = []\n",
    "with open(\"works_cycles_train.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "trainset = dataset[:100]\n",
    "testset = dataset[100:200]\n",
    "\n",
    "table_schema = generate_schema_prompt_sqlite(\"works_cycles/works_cycles.sqlite\")\n",
    "\n",
    "from ape.common.types import DatasetItem\n",
    "\n",
    "\n",
    "trainset = [DatasetItem(inputs={\"question\": item[\"question\"], \"table_schema\": table_schema}, outputs={\"answer\": item[\"SQL\"]}) for item in trainset]\n",
    "testset = [DatasetItem(inputs={\"question\": item[\"question\"], \"table_schema\": table_schema}, outputs={\"answer\": item[\"SQL\"]}) for item in testset]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prompt to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import Prompt\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "Using valid SQLite, answer the following questions for the tables provided above.\n",
    "Question: {question}\n",
    "Table Schema: {table_schema}\n",
    "Generate the SQLite for the above question after thinking step by step.\n",
    "\"\"\"\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"json_schema\", \n",
    "    \"json_schema\": {\n",
    "        \"name\": \"TextToSQL\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"thought\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The reasoning process of the problem solving\"\n",
    "                },\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The answer SQL query for question\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"thought\", \"answer\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "student_prompt = Prompt(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    response_format=json_schema,\n",
    "    name=\"SQL Generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Generator, Metric, and Global Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common.generator import BaseGenerator\n",
    "from ape.common.metric import BaseMetric\n",
    "from ape.common.global_metric import BaseGlobalMetric\n",
    "from ape.common.types import MetricResult, GlobalMetricResult\n",
    "\n",
    "# define generator, metric, global metric\n",
    "\n",
    "db_path = \"works_cycles/works_cycles.sqlite\"\n",
    "openai = AsyncOpenAI()\n",
    "\n",
    "class BirdBenchSolver(BaseGenerator):\n",
    "    async def generate(\n",
    "        self,\n",
    "        prompt: Prompt,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Union[Dict[str, Any], str]:\n",
    "        retry_count = 0\n",
    "        messages = prompt.format(**inputs).messages\n",
    "        model = prompt.model\n",
    "        response_format = prompt.response_format\n",
    "        while retry_count < 3:\n",
    "            try:\n",
    "                response = await openai.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format=response_format,\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                return json.loads(response.choices[0].message.content)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retry_count += 1\n",
    "        return {\n",
    "            \"thought\": \"error\",\n",
    "            \"answer\": \"\",\n",
    "        }\n",
    "\n",
    "\n",
    "class BirdBenchMetric(BaseMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        dataset_item: DatasetItem,\n",
    "        pred: Dict[str, Any],\n",
    "    ) -> MetricResult:\n",
    "        try:\n",
    "            pred_sql = pred[\"answer\"]\n",
    "            ground_truth_sql = dataset_item[\"outputs\"][\"answer\"]\n",
    "            \n",
    "            pred_rows = execute_sql(pred_sql, db_path)\n",
    "            ground_truth_rows = execute_sql(ground_truth_sql, db_path)\n",
    "            \n",
    "            exact_match_score = calculate_exact_match(pred_rows, ground_truth_rows)\n",
    "            f1_score = calculate_f1_score(pred_rows, ground_truth_rows)\n",
    "            # ves = calculate_velocity_efficiency_score(pred_sql, ground_truth_sql, db_path, 10)\n",
    "            \n",
    "            # score = (exact_match_score + f1_score + ves) / 3\n",
    "            score = (exact_match_score + f1_score) / 2\n",
    "            return MetricResult(\n",
    "                score=score,\n",
    "                trace={\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"exact_match_score\": exact_match_score,\n",
    "                    # \"velocity_efficiency_score\": ves\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # print(pred)\n",
    "            return MetricResult(\n",
    "                score=0.0,\n",
    "                trace={\n",
    "                    # \"velocity_efficiency_score\": 0.0,\n",
    "                    \"f1_score\": 0.0,\n",
    "                    \"exact_match_score\": 0.0\n",
    "                }\n",
    "            )\n",
    "\n",
    "class GlobalAverageMetric(BaseGlobalMetric):\n",
    "    async def compute(\n",
    "        self,\n",
    "        results: List[MetricResult],\n",
    "    ) -> GlobalMetricResult:\n",
    "        try:\n",
    "            scores = [result.score for result in results]\n",
    "            exact_match_scores = [result.trace[\"exact_match_score\"] for result in results]\n",
    "            f1_scores = [result.trace[\"f1_score\"] for result in results]\n",
    "            # ves_scores = [result.trace[\"velocity_efficiency_score\"] for result in results]    \n",
    "            return GlobalMetricResult(\n",
    "                score=sum(scores) / len(scores) if len(results) > 0 else 0.0,\n",
    "                trace={\n",
    "                    \"exact_match_scores\": sum(exact_match_scores) / len(exact_match_scores),\n",
    "                    \"f1_scores\": sum(f1_scores) / len(f1_scores),\n",
    "                    # \"ves_scores\": sum(ves_scores) / len(ves_scores)\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # print(\"Error in GlobalEmotionMetric: \", e)\n",
    "            return GlobalMetricResult(\n",
    "                score=0.0,\n",
    "                trace={\n",
    "                    \"exact_match_scores\": 0.0,\n",
    "                    \"f1_scores\": 0.0,\n",
    "                    # \"ves_scores\": 0.0\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Trainer & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.core.trainer import (\n",
    "    TextGradientTrainer,\n",
    "    ExpelTrainer,\n",
    "    FewShotTrainer,\n",
    "    EvoPromptTrainer,\n",
    "    DspyMiproTrainer,\n",
    "    OptunaTrainer,\n",
    ")\n",
    "\n",
    "# define trainer \n",
    "trainer = FewShotTrainer(\n",
    "    generator=BirdBenchSolver(),\n",
    "    metric=BirdBenchMetric(),\n",
    "    global_metric=GlobalAverageMetric(),\n",
    "    testmode=True # If True, trainer will run prompts for validation set and save results.\n",
    ")\n",
    "\n",
    "# run trainer\n",
    "optimized_prompt, report = await trainer.train(\n",
    "    prompt=student_prompt,\n",
    "    trainset=trainset,\n",
    "    valset=testset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimized prompt\n",
    "for message in optimized_prompt.messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Benchmark Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize experiment results\n",
    "def visualize_scores(report):\n",
    "    scores = report.scores\n",
    "    trainset_scores = [score[\"score\"] for score in scores]\n",
    "    valset_scores = [score[\"val_score\"] for score in scores]\n",
    "    iterations = range(1, len(trainset_scores) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, trainset_scores, label='Training Set', marker='o')\n",
    "    plt.plot(iterations, valset_scores, label='Validation Set', marker='s')\n",
    "    \n",
    "    for i, (train_score, val_score) in enumerate(zip(trainset_scores, valset_scores)):\n",
    "        plt.text(iterations[i], train_score, f'{train_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='blue')\n",
    "        plt.text(iterations[i], val_score, f'{val_score:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, color='green')\n",
    "\n",
    "    plt.title('Training and Validation Scores over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scores(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
