# OptunaTrainer

## Overview

(Note: The name of this trainer will be updated soon.)  
OptunaTrainer is inspired by [DSPy](https://github.com/stanfordnlp/DSPy) but focuses more on instruction optimization.

If you want to see more details like explanation of the algorithm and difference between implementation and paper, please refer to [here](https://github.com/weavel-ai/Ape/libs/ape-core/ape/core/trainer/community/optuna/README.md).

## Methods

### `__init__`

It has the same parameters as [`Trainer`](../../trainer.mdx) class, but have some more unique parameters.

**unique parameters:**

- `num_candidates`: Number of candidate few-shot example groups to generate.
- `minibatch_size`: Size of minibatches for evaluation.
- `max_steps`: Maximum number of optimization steps.
