# DSPy-Mipro

## Overview

**Dspy-Mipro** is an algorithm from paper [DSPy](https://arxiv.org/abs/2310.03714) and project [DSPy](https://github.com/stanfordnlp/DSPy), based on the MiproV2 optimizer, implemented as of 2024.10.21.

If you want to see more details like explanation of the algorithm and difference between implementation and paper, please refer to [here](https://github.com/weavel-ai/Ape/libs/ape-core/ape/core/trainer/papers/dspy_mipro/README.md).

## Methods

### `__init__`

It has the same parameters as [`Trainer`](../../trainer.mdx) class, but have some more unique parameters.

**unique parameters:**

- `num_candidates`: Number of candidate prompts to generate.
- `max_steps`: Maximum number of optimization steps.
- `minibatch_size`: Size of minibatches for evaluation.
- `max_bootstrapped_demos`: Maximum number of bootstrapped examples. (examples with generated internal steps)
- `max_labeled_demos`: Maximum number of labeled examples. (examples selected from `trainset`)
- `success_score`: score threshold to determine if model is successful to solve the task.
